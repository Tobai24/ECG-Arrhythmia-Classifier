{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps in building a machine learning model\n",
    "\n",
    "1. Get the Data \n",
    "\n",
    "2. Take a Quick Look at the Data \n",
    "\n",
    "3. Split the Data (Train/Validation/Test) \n",
    "\n",
    "4. Data Analysis (Exploratory Data Analysis or EDA) \n",
    "\n",
    "5. Prepare the Data for Machine Learning  \n",
    "\n",
    "6. Select and Train a Model  \n",
    "\n",
    "7. Evaluate the Model \n",
    "\n",
    "8. Fine-Tune the Model \n",
    "\n",
    "9. Select a final model\n",
    "\n",
    "10. Deployment\n",
    "\n",
    "_Step 1-5 would be done in this notebook step 6-7 will be done in the [experiment.ipynb](./experiment.ipynb) notebook step 8-9 will be done in the [tuning folder](../tuning/) step 10 would be done in the [deployment folder](../deployment/)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Get the Data**  \n",
    "   - Gather the dataset from relevant sources (e.g., Kaggle, databases, APIs, etc.). I have done this earlier you can check the [data folder](../data/) for more details.\n",
    "   - Here we would just load the data using pandas. so first we have to import the necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Take a Quick Look at the Data**  \n",
    "   - Perform initial exploration of the dataset (e.g., inspect the data types, check for missing values, and view a few records).\n",
    "   - This helps to understand what you’re working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Split the Data (Train/Validation/Test)**  \n",
    "   - Split the data into training, validation, and test sets to evaluate the model’s performance.\n",
    "   - A typical split might be 70% training, 15% validation, and 15% testing, though it can vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Data Analysis (Exploratory Data Analysis or EDA)**  \n",
    "   - Use statistical and visualization techniques to understand the data better.\n",
    "   - This includes checking distributions, correlations, and trends, as well as identifying outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Prepare the Data for Machine Learning**  \n",
    "   - Clean the data (e.g., handling missing values, scaling, normalization, encoding categorical variables).\n",
    "   - Feature engineering may also be needed (e.g., creating new features from existing ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
